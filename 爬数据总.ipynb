{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7-dWYL-oZgc1"
      },
      "outputs": [],
      "source": [
        "!pip install praw"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import praw\n",
        "import pandas as pd"
      ],
      "metadata": {
        "id": "Br2s2t5F8JGs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "\n",
        "secret =   {'client_id':\"zBSC5ziXXikuqD_PpmYCzw\",\n",
        "             'client_secret':\"JXrpx0UErzYqpsmqCOXgZeDBZ1GLuA\",\n",
        "             'password':\"duyichen131\",\n",
        "             'user_agent':\"attitude towards ai\",\n",
        "             'username':\"zimm_yichen\",\n",
        "            }\n",
        "\n",
        "with open(\"secret.json\", \"w\") as f:\n",
        "       json.dump(secret, f)"
      ],
      "metadata": {
        "id": "5GMGOiDuYLIx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "with open(\"secret.json\", \"r\") as f:\n",
        "    secret = json.load(f)"
      ],
      "metadata": {
        "id": "CjQ0qVwQY8EC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "reddit = praw.Reddit(\n",
        "    client_id = secret['client_id'],\n",
        "    client_secret = secret['client_secret'],\n",
        "    username = secret['username'],\n",
        "    password = secret['password'],\n",
        "    user_agent = secret['user_agent']\n",
        ")"
      ],
      "metadata": {
        "id": "0Lr_cyUvZNQf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def fetch_posts_from_subreddit(subreddit_name, limit=1000):\n",
        "    subreddit = reddit.subreddit(subreddit_name)\n",
        "    post_list = []\n",
        "    keyword = ['AI', 'AIs', 'artificial intelligence', 'Machine Learning', 'llm']\n",
        "    counter = 1  # Initialize a counter\n",
        "\n",
        "    for post in subreddit.top(limit=limit):\n",
        "        for kw in keyword:\n",
        "            if kw.lower() in post.title.lower() or kw.lower() in post.selftext.lower():\n",
        "                # Check if post.author is not None before accessing .name\n",
        "                author_name = post.author.name if post.author else \"Unknown\"\n",
        "                post_info = {\n",
        "                    'id': counter,  # Add the counter as an 'id'\n",
        "                    'title': post.title,\n",
        "                    'content': post.selftext,\n",
        "                    'created_utc': post.created_utc,\n",
        "                    'author': author_name,  # Use the author_name variable\n",
        "                    'url': post.url  # Add post's URL\n",
        "                }\n",
        "                post_list.append(post_info)\n",
        "                counter += 1  # Increment the counter\n",
        "    return post_list"
      ],
      "metadata": {
        "id": "zJczcFqg1QCu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "singularity_posts = fetch_posts_from_subreddit('singularity', limit=1000)\n",
        "aiwars_posts = fetch_posts_from_subreddit('aiwars', limit=200)"
      ],
      "metadata": {
        "id": "hKxtzpTF2Pxl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#将两个社区的帖子合并到一个csv\n",
        "combined_posts = singularity_posts + aiwars_posts\n",
        "\n",
        "# 将合并后的帖子列表转换成DataFrame\n",
        "df_combined = pd.DataFrame(combined_posts)\n",
        "\n",
        "# 将'created_utc'列转换成日期格式\n",
        "df_combined['created_utc'] = pd.to_datetime(df_combined['created_utc'], utc=True, unit='s').dt.date\n",
        "\n",
        "# 导出到CSV，不包含索引列\n",
        "df_combined.to_csv('combined_posts.csv', index=False)\n"
      ],
      "metadata": {
        "id": "2UAC2ffy3tYp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "non_empty_contents_count = df_combined['content'].replace('', pd.NA).dropna().shape[0]\n",
        "\n",
        "print(f'Numner of Ccntents: {non_empty_contents_count}')"
      ],
      "metadata": {
        "id": "HD58hYV_79h3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import prawcore\n",
        "from datetime import datetime\n",
        "\n",
        "def is_valid_reddit_url(url):\n",
        "    # 检查URL是否是有效的Reddit帖子URL\n",
        "    return url.startswith('https://www.reddit.com/r/') or url.startswith('https://reddit.com/r/')\n",
        "\n",
        "def expand_comments(submission):\n",
        "    # 将所有MoreComments对象替换为实际的评论对象\n",
        "    submission.comments.replace_more(limit=0)\n",
        "\n",
        "combined_comments = []\n",
        "\n",
        "for index, post_info in df_combined.iterrows():\n",
        "    url = post_info['url']\n",
        "    if not is_valid_reddit_url(url):\n",
        "        continue  # 跳过这个URL，继续下一个\n",
        "\n",
        "    try:\n",
        "        # 使用praw获取submission对象\n",
        "        submission_id = url.split('/')[-3]\n",
        "        submission = reddit.submission(id=submission_id)\n",
        "\n",
        "        # 展开评论\n",
        "        expand_comments(submission)\n",
        "\n",
        "        # 收集评论信息\n",
        "        for comment in submission.comments.list():\n",
        "            combined_comments.append({\n",
        "                'post_id': post_info['id'],\n",
        "                'comment_author': comment.author.name if comment.author else 'Deleted',\n",
        "                'comment_body': comment.body,\n",
        "                'comment_created_utc': datetime.utcfromtimestamp(comment.created_utc).isoformat()\n",
        "            })\n",
        "    except prawcore.exceptions.NotFound:\n",
        "        pass\n",
        "    except Exception as e:\n",
        "        pass\n",
        "print(f'总共爬取了 {len(combined_comments)} 条评论。')\n",
        "\n",
        "df_all_comments = pd.DataFrame(combined_comments)\n",
        "df_all_comments.to_csv('combined_comments.csv', index=False)\n"
      ],
      "metadata": {
        "id": "oHsILR5LTasU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n"
      ],
      "metadata": {
        "id": "zaH_Vk0IT-_R",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "16220538-e2b4-4144-dcb1-c18182ca61f9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!cp combined_posts.csv \"/content/drive/My Drive/Colab Notebooks\"\n",
        "!cp combined_comments.csv \"/content/drive/My Drive/Colab Notebooks\"\n"
      ],
      "metadata": {
        "id": "-zsLJpAjVenv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.decomposition import LatentDirichletAllocation\n",
        "from sklearn.feature_extraction import text\n",
        "import matplotlib.pyplot as plt\n"
      ],
      "metadata": {
        "id": "BBOvvuXyiRZF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "lsLSMte8iR5M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "posts = pd.read_csv('/content/drive/My Drive/Colab Notebooks/combined_posts.csv', usecols=['processed_title', 'processed_content'])\n",
        "comments = pd.read_csv('/content/drive/My Drive/Colab Notebooks/combined_comments.csv', usecols=['processed_comment_body'])\n",
        "\n",
        "posts['text'] = posts['processed_title'] + \" \" + posts['processed_content']\n",
        "comments.rename(columns={'processed_comment_body': 'text'}, inplace=True)\n",
        "\n",
        "data = pd.concat([posts['text'], comments['text']], axis=0).reset_index(drop=True)\n",
        "\n",
        "data_clean = data.dropna().reset_index(drop=True)"
      ],
      "metadata": {
        "id": "DTCO3fnJiUQY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "my_stop_words = list(text.ENGLISH_STOP_WORDS.union(['ai','think','may','thing','way','want','like','use','know','make']))\n",
        "vectorizer = CountVectorizer(max_df=0.95, min_df=2, stop_words=my_stop_words)\n",
        "data_vectorized = vectorizer.fit_transform(data_clean)\n",
        "\n",
        "# 应用 LDA\n",
        "lda = LatentDirichletAllocation(n_components=5, random_state=42)\n",
        "lda.fit(data_vectorized)\n",
        "\n",
        "# 文档主题分布\n",
        "document_topics = lda.transform(data_vectorized)\n",
        "\n",
        "# 选择每个文档的主要主题\n",
        "topics = document_topics.argmax(axis=1)\n",
        "\n",
        "# 将主题编号加入到 DataFrame 中\n",
        "data_clean = pd.DataFrame(data_clean)  # 确保 data_clean 是 DataFrame\n",
        "data_clean['Topic'] = topics"
      ],
      "metadata": {
        "id": "P5OseLJriXKP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def display_topics(model, feature_names, no_top_words):\n",
        "    for topic_idx, topic in enumerate(model.components_):\n",
        "        print(f\"\\nTopic {topic_idx}:\")\n",
        "        print(\" \".join([feature_names[i] for i in topic.argsort()[:-no_top_words - 1:-1]]))\n",
        "\n",
        "print(\"\\nLDA Model Topics and their Keywords:\")\n",
        "display_topics(lda, vectorizer.get_feature_names_out(), 10)"
      ],
      "metadata": {
        "id": "YSRSEUGgiZU_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install gensim\n",
        "\n",
        "from gensim.corpora import Dictionary\n",
        "from gensim.models import LdaModel\n",
        "from gensim.models.coherencemodel import CoherenceModel\n",
        "\n",
        "# Define custom stop words\n",
        "custom_stop_words = ['think','may','thing','way','want','like','use','know','make']\n",
        "\n",
        "# Tokenize and remove stop words\n",
        "data_tokenized = [[word for word in str(text).lower().split() if word not in custom_stop_words] for text in data]\n",
        "\n",
        "# Create a Gensim dictionary\n",
        "dictionary = Dictionary(data_tokenized)\n",
        "\n",
        "# Create a Gensim corpus\n",
        "corpus = [dictionary.doc2bow(text) for text in data_tokenized]\n",
        "\n",
        "# Use Gensim's LDA\n",
        "lda_model = LdaModel(corpus=corpus, id2word=dictionary, num_topics=5, random_state=42, passes=20)\n",
        "\n",
        "# Compute Coherence Score\n",
        "coherence_model_lda = CoherenceModel(model=lda_model, texts=data_tokenized, dictionary=dictionary, coherence='c_v')\n",
        "coherence_lda = coherence_model_lda.get_coherence()\n",
        "print('\\nCoherence Score: ', coherence_lda)"
      ],
      "metadata": {
        "id": "Ey4wsHF2ia5X"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "doc_topic_dist = [lda_model.get_document_topics(doc_bow) for doc_bow in corpus]\n",
        "\n",
        "# Select your topic of interest\n",
        "selected_topic = 2\n",
        "topic_relevance = []\n",
        "\n",
        "# Extract the relevance of the selected topic for each document\n",
        "for doc_distribution in doc_topic_dist:\n",
        "    doc_topic_relevance = next((score for topic_id, score in doc_distribution if topic_id == selected_topic), 0)\n",
        "    topic_relevance.append(doc_topic_relevance)\n",
        "\n",
        "# Sort documents by their relevance to the selected topic\n",
        "import numpy as np\n",
        "topic_relevance = np.array(topic_relevance)\n",
        "top_doc_indices = topic_relevance.argsort()[::-1][:10]  # Indices of the top 10 documents\n",
        "\n",
        "# Print the most relevant documents to the selected topic\n",
        "print(f\"Top documents for Topic {selected_topic}:\\n\")\n",
        "for index in top_doc_indices:\n",
        "    print(f\"Document {index}:\\n{data.iloc[index]}\\n\")\n",
        "    print(\"--------------------------------------------------\\n\")"
      ],
      "metadata": {
        "id": "oGVJcFwtiej7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "analyzer = SentimentIntensityAnalyzer()\n",
        "\n",
        "def analyze_sentiment_vader(text):\n",
        "    return analyzer.polarity_scores(text)['compound']\n",
        "\n",
        "data_clean['Sentiment_Score'] = data_clean['text'].apply(analyze_sentiment_vader)\n",
        "\n",
        "average_sentiment_per_topic = data_clean.groupby('Topic')['Sentiment_Score'].mean()\n",
        "print(average_sentiment_per_topic)"
      ],
      "metadata": {
        "id": "A9Uock9KigKn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "colors = ['#a1c9f4', '#ffb3ba', '#baffc9', '#f8b878', '#f0b7a4']\n",
        "\n",
        "for i, topic_number in enumerate(sorted(data_clean['Topic'].unique())):\n",
        "    plt.figure(figsize=(10, 6))\n",
        "    subset = data_clean[data_clean['Topic'] == topic_number]\n",
        "\n",
        "    sns.histplot(subset['Sentiment_Score'], kde=False, bins=30, color=colors[i])\n",
        "    plt.title(f'Topic {topic_number+1} Sentiment Distribution')\n",
        "    plt.xlabel('Sentiment Score')\n",
        "    plt.ylabel('Count')\n",
        "    plt.show()"
      ],
      "metadata": {
        "id": "kxbIlhpNihh-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure(figsize=(10, 6))\n",
        "\n",
        "for topic_number in sorted(data_clean['Topic'].unique()):\n",
        "    subset = data_clean[data_clean['Topic'] == topic_number]\n",
        "\n",
        "    sns.kdeplot(subset['Sentiment_Score'], label=f'Topic {topic_number}', bw_adjust=0.5)\n",
        "\n",
        "plt.title('Sentiment Score Density by Topic')\n",
        "plt.xlabel('Sentiment Score')\n",
        "plt.ylabel('Density')\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "rMG990ERiiu4"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}